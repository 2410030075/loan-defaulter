{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535502b5",
   "metadata": {},
   "source": [
    "# Loan Default Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs an exploratory data analysis (EDA) of the Loan Default dataset, examining features, distributions, relationships, and preparing insights for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbdb8d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style and size\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3c612",
   "metadata": {},
   "source": [
    "## 2. Load and Examine the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e459e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d65bcfab",
   "metadata": {},
   "source": [
    "### 2.1 Load the Dataset and Display Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the expected dataset path\n",
    "data_path = '../data/Loan_Default.csv'\n",
    "alternative_path = '../data/loan_default.csv'\n",
    "\n",
    "# Check if file exists (case-sensitive check)\n",
    "file_exists = os.path.exists(data_path) or os.path.exists(alternative_path)\n",
    "\n",
    "# Set the actual path based on what exists\n",
    "actual_path = data_path if os.path.exists(data_path) else alternative_path if os.path.exists(alternative_path) else None\n",
    "\n",
    "if actual_path:\n",
    "    print(f\"✅ Dataset found at: {os.path.abspath(actual_path)}\")\n",
    "    \n",
    "    # Get file size in MB\n",
    "    file_size_bytes = os.path.getsize(actual_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Try to load the dataset with different encodings if necessary\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    df = None\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(actual_path, encoding=encoding)\n",
    "            print(f\"Successfully loaded dataset with encoding: {encoding}\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to load with encoding: {encoding}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    if df is not None:\n",
    "        # Display basic information\n",
    "        print(f\"\\nDataset shape: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        print(\"\\nColumn list:\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"{i}. {col}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load the dataset with any encoding.\")\n",
    "else:\n",
    "    print(\"❌ Dataset not found.\")\n",
    "    print(\"To download the dataset manually from Kaggle:\")\n",
    "    print(\"1. Visit https://www.kaggle.com/datasets/yasserh/loan-default-dataset\")\n",
    "    print(\"2. Click 'Download' button\")\n",
    "    print(\"3. Save the file to the 'data' folder as 'Loan_Default.csv'\")\n",
    "    \n",
    "    print(\"\\nTo download using Kaggle CLI:\")\n",
    "    print(\"1. Install Kaggle CLI: pip install kaggle\")\n",
    "    print(\"2. Set up your Kaggle API token in ~/.kaggle/kaggle.json\")\n",
    "    print(\"3. Run the following command:\")\n",
    "    print(\"   kaggle datasets download -d yasserh/loan-default-dataset --path ../data\")\n",
    "    print(\"4. Unzip the downloaded file:\")\n",
    "    print(\"   Expand-Archive -Path ../data/loan-default-dataset.zip -DestinationPath ../data\")\n",
    "    print(\"5. Verify the file exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe47ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Generate a summary report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Count data types\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    print(f\"Data types in dataset:\")\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"- {dtype}: {count} columns\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    print(f\"\\nMemory usage: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    # Missing values overview\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_cols = missing_values[missing_values > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\nColumns with missing values: {len(missing_cols)}\")\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"- {col}: {count} missing values ({count/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found in the dataset.\")\n",
    "        \n",
    "    # Data summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMPLETE SUMMARY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"File path: {os.path.abspath(actual_path)}\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    print(f\"Column list: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/Loan_Default.csv'\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    \n",
    "    # Display first 10 rows\n",
    "    print(\"\\nFirst 10 rows of the dataset:\")\n",
    "    display(df.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b495d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and null counts\n",
    "print(\"Data types and null counts:\")\n",
    "buffer = []\n",
    "for col in df.columns:\n",
    "    dtype = df[col].dtype\n",
    "    nulls = df[col].isnull().sum()\n",
    "    null_percent = nulls / len(df) * 100\n",
    "    buffer.append({\n",
    "        'Column': col,\n",
    "        'Data Type': dtype,\n",
    "        'Null Count': nulls,\n",
    "        'Null %': f\"{null_percent:.2f}%\"\n",
    "    })\n",
    "\n",
    "dtypes_df = pd.DataFrame(buffer)\n",
    "display(dtypes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a024",
   "metadata": {},
   "source": [
    "### 2.2 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable - in this dataset, it's 'Status' column\n",
    "# Check if 'default' or similar columns exist\n",
    "target_candidates = ['default', 'loan_default', 'Status']\n",
    "target_col = None\n",
    "\n",
    "for col in target_candidates:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"Target variable identified: '{target_col}'\")\n",
    "    \n",
    "    # Count distribution\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    target_percents = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    target_df = pd.DataFrame({\n",
    "        'Count': target_counts,\n",
    "        'Percentage': target_percents\n",
    "    })\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    display(target_df)\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(x=target_col, data=df, palette=['#66b3ff', '#ff9999'])\n",
    "    plt.title(f'Distribution of {target_col}', fontsize=16)\n",
    "    plt.xlabel(target_col, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add count and percentage labels above bars\n",
    "    for i, p in enumerate(target_percents):\n",
    "        plt.text(i, target_counts.values[i] + len(df)*0.01, \n",
    "                 f\"{target_counts.values[i]}\\n({p:.1f}%)\", \n",
    "                 ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target variable not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a5bdc",
   "metadata": {},
   "source": [
    "### 2.3 Summary Statistics for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove ID columns and target if it's numerical\n",
    "exclude_cols = ['ID', 'id', target_col] if target_col else ['ID', 'id']\n",
    "numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "if numerical_cols:\n",
    "    # Calculate summary statistics\n",
    "    summary_stats = df[numerical_cols].describe().T\n",
    "    \n",
    "    # Add median to the summary\n",
    "    summary_stats['median'] = df[numerical_cols].median()\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    summary_stats = summary_stats[['count', 'mean', 'median', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "    \n",
    "    print(\"Summary statistics for numerical features:\")\n",
    "    display(summary_stats)\n",
    "else:\n",
    "    print(\"No numerical columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313628d",
   "metadata": {},
   "source": [
    "### 2.4 Detect Suspicious Placeholder Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb122296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define potential suspicious values to check\n",
    "suspicious_values = [-1, -99, -999, -9999, 9999, 99999, 999999]\n",
    "\n",
    "# Store columns with suspicious values\n",
    "suspicious_columns = {}\n",
    "\n",
    "# Check numerical columns for suspicious values\n",
    "for col in numerical_cols:\n",
    "    # Get unique values\n",
    "    unique_vals = df[col].unique()\n",
    "    \n",
    "    # Check if any suspicious values are present in high frequency\n",
    "    for val in suspicious_values:\n",
    "        if val in unique_vals:\n",
    "            # Calculate percentage of this value\n",
    "            val_percentage = (df[col] == val).mean() * 100\n",
    "            \n",
    "            # Only consider it suspicious if it appears with some frequency (>0.5%)\n",
    "            if val_percentage > 0.5:\n",
    "                if col not in suspicious_columns:\n",
    "                    suspicious_columns[col] = []\n",
    "                suspicious_columns[col].append((val, val_percentage))\n",
    "\n",
    "# Display results\n",
    "if suspicious_columns:\n",
    "    print(\"Suspicious placeholder values detected:\")\n",
    "    for col, values in suspicious_columns.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        for val, percentage in values:\n",
    "            print(f\"  - Value {val} appears {percentage:.2f}% of the time\")\n",
    "else:\n",
    "    print(\"No suspicious placeholder values detected in the numerical columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505c29c",
   "metadata": {},
   "source": [
    "### 2.5 Distribution of Key Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3367b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features to look for\n",
    "key_feature_names = ['income', 'loan_amount', 'credit_score', 'Credit_Score', 'loan_amt', 'interest_rate']\n",
    "\n",
    "# Find matching columns in our dataset\n",
    "key_features = [col for col in numerical_cols if col.lower() in [name.lower() for name in key_feature_names]]\n",
    "\n",
    "# If no matches, take first few numerical columns\n",
    "if not key_features and numerical_cols:\n",
    "    key_features = numerical_cols[:3]\n",
    "    print(f\"No exact matches for key features found. Using first 3 numerical columns: {key_features}\")\n",
    "else:\n",
    "    print(f\"Found key numerical features: {key_features}\")\n",
    "\n",
    "# Plot histograms with KDE for key features\n",
    "if key_features:\n",
    "    plt.figure(figsize=(18, 5 * ((len(key_features) + 2) // 3)))\n",
    "    for i, col in enumerate(key_features, 1):\n",
    "        plt.subplot((len(key_features) + 2) // 3, 3, i)\n",
    "        sns.histplot(df[col].dropna(), kde=True)\n",
    "        plt.title(f'Distribution of {col}', fontsize=14)\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical features found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a07079",
   "metadata": {},
   "source": [
    "### 2.6 Outlier Detection in Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for key numerical features to detect outliers\n",
    "if key_features:\n",
    "    plt.figure(figsize=(18, 5 * ((len(key_features) + 2) // 3)))\n",
    "    for i, col in enumerate(key_features, 1):\n",
    "        plt.subplot((len(key_features) + 2) // 3, 3, i)\n",
    "        sns.boxplot(y=df[col].dropna())\n",
    "        plt.title(f'Boxplot of {col} (Outlier Detection)', fontsize=14)\n",
    "        plt.ylabel(col, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display IQR statistics for key features\n",
    "    outlier_stats = []\n",
    "    for col in key_features:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col].count()\n",
    "        outliers_percentage = outliers_count / df[col].count() * 100\n",
    "        \n",
    "        outlier_stats.append({\n",
    "            'Feature': col,\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR,\n",
    "            'Lower Bound': lower_bound,\n",
    "            'Upper Bound': upper_bound,\n",
    "            'Outliers Count': outliers_count,\n",
    "            'Outliers %': f\"{outliers_percentage:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    print(\"Outlier Statistics:\")\n",
    "    display(pd.DataFrame(outlier_stats))\n",
    "else:\n",
    "    print(\"No numerical features found for outlier detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb022493",
   "metadata": {},
   "source": [
    "### 2.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "if len(numerical_cols) > 1:\n",
    "    # Include the target if it's numerical\n",
    "    if target_col and target_col in df.columns and df[target_col].dtype in ['int64', 'float64']:\n",
    "        cols_for_corr = numerical_cols + [target_col]\n",
    "    else:\n",
    "        cols_for_corr = numerical_cols\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df[cols_for_corr].corr()\n",
    "    \n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', \n",
    "                linewidths=0.5, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get top correlated pairs (excluding self-correlations)\n",
    "    corr_pairs = []\n",
    "    \n",
    "    # Extract correlation pairs\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            correlation = corr_matrix.iloc[i, j]\n",
    "            corr_pairs.append((col_i, col_j, abs(correlation), correlation))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Print top 10 correlated pairs\n",
    "    print(\"Top 10 Correlated Feature Pairs:\")\n",
    "    for i, (col1, col2, abs_corr, corr) in enumerate(corr_pairs[:10], 1):\n",
    "        print(f\"{i}. {col1} — {col2}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough numerical columns for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493d4d5",
   "metadata": {},
   "source": [
    "### 2.8 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad951689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if categorical_cols:\n",
    "    # Calculate cardinality (number of unique values) for each categorical column\n",
    "    cat_cardinality = []\n",
    "    for col in categorical_cols:\n",
    "        unique_values = df[col].nunique()\n",
    "        example_values = ', '.join(df[col].dropna().unique()[:3].astype(str)) + '...'\n",
    "        cat_cardinality.append({\n",
    "            'Column': col, \n",
    "            'Cardinality': unique_values,\n",
    "            'Example Values': example_values\n",
    "        })\n",
    "    \n",
    "    # Display cardinality information\n",
    "    cat_card_df = pd.DataFrame(cat_cardinality).sort_values('Cardinality', ascending=False)\n",
    "    print(\"Categorical columns and their cardinality (number of unique values):\")\n",
    "    display(cat_card_df)\n",
    "    \n",
    "    # Plot distribution for categorical columns with low cardinality\n",
    "    low_card_cols = [col for col in categorical_cols if df[col].nunique() <= 10]\n",
    "    \n",
    "    if low_card_cols:\n",
    "        for col in low_card_cols[:5]:  # Limit to 5 columns to avoid too many plots\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts = df[col].value_counts().sort_values(ascending=False)\n",
    "            \n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "            plt.title(f'Distribution of {col}', fontsize=14)\n",
    "            plt.xlabel(col, fontsize=12)\n",
    "            plt.ylabel('Count', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            # Add percentage labels\n",
    "            total = len(df)\n",
    "            for i, v in enumerate(value_counts):\n",
    "                plt.text(i, v + total*0.01, f\"{v} ({v/total*100:.1f}%)\", ha='center')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No categorical columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6b400",
   "metadata": {},
   "source": [
    "### 2.9 Save EDA Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure results directory exists\n",
    "import os\n",
    "results_dir = os.path.join('..', 'results')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# 1. Save missing values information to CSV\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percent})\n",
    "missing_csv_path = os.path.join(results_dir, 'missing_values.csv')\n",
    "missing_df.to_csv(missing_csv_path)\n",
    "print(f\"Missing values data saved to {missing_csv_path}\")\n",
    "\n",
    "# 2. Save EDA report as HTML\n",
    "try:\n",
    "    import pandas_profiling\n",
    "    from pandas_profiling import ProfileReport\n",
    "    \n",
    "    # Generate profile report\n",
    "    profile = ProfileReport(df, title=\"Loan Default Dataset Profiling Report\", minimal=True)\n",
    "    \n",
    "    # Save report to file\n",
    "    eda_report_path = os.path.join(results_dir, 'eda_report.html')\n",
    "    profile.to_file(eda_report_path)\n",
    "    print(f\"EDA report saved to {eda_report_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"pandas-profiling not installed. Installing it would allow generation of comprehensive EDA reports.\")\n",
    "    print(\"Install with: pip install pandas-profiling\")\n",
    "    \n",
    "    # Alternative: save a basic HTML summary\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a simple HTML report\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Loan Default EDA Summary</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Loan Default Dataset EDA Summary</h1>\n",
    "        <p>Dataset shape: {df.shape[0]} rows and {df.shape[1]} columns</p>\n",
    "        \n",
    "        <h2>Data Types</h2>\n",
    "        {df.dtypes.value_counts().to_frame().to_html()}\n",
    "        \n",
    "        <h2>Missing Values Summary</h2>\n",
    "        {missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False).to_html()}\n",
    "        \n",
    "        <h2>Numerical Features Summary</h2>\n",
    "        {df.describe().to_html()}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the HTML file\n",
    "    eda_report_path = os.path.join(results_dir, 'eda_report.html')\n",
    "    with open(eda_report_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"Simple EDA report saved to {eda_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46339c06",
   "metadata": {},
   "source": [
    "## 3. EDA Summary\n",
    "\n",
    "This exploratory data analysis has:\n",
    "\n",
    "1. Loaded and verified the Loan Default dataset\n",
    "2. Analyzed the target variable distribution\n",
    "3. Calculated summary statistics for numerical features\n",
    "4. Detected suspicious placeholder values\n",
    "5. Visualized distributions of key numerical features\n",
    "6. Identified outliers in numerical features\n",
    "7. Analyzed correlations between features\n",
    "8. Examined categorical features and their cardinality\n",
    "9. Saved EDA outputs to files\n",
    "\n",
    "Key insights:\n",
    "- The dataset has 148,670 rows and 34 columns\n",
    "- 14 columns contain missing values (ranging from 0.03% to 26.66%)\n",
    "- The target variable is \"Status\" with binary values (0/1)\n",
    "- Several numerical features show significant outliers\n",
    "- The dataset contains both numerical and categorical features\n",
    "\n",
    "Next steps:\n",
    "- Apply appropriate preprocessing based on these findings\n",
    "- Handle missing values and outliers\n",
    "- Encode categorical features\n",
    "- Select features based on correlation analysis\n",
    "- Apply feature engineering techniques\n",
    "- Prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5696dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary of numerical variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c887f",
   "metadata": {},
   "source": [
    "## 3. Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68369b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "print(\"Missing Values:\")\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b062884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values if there are any\n",
    "if len(missing_df) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(missing_df.index, missing_df['Percentage'], color='crimson')\n",
    "    plt.title('Percentage of Missing Values by Column', fontsize=16)\n",
    "    plt.xlabel('Columns', fontsize=12)\n",
    "    plt.ylabel('Percentage', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58f827",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'loan_default' is the target variable, check its distribution\n",
    "if 'loan_default' in df.columns:\n",
    "    target_col = 'loan_default'\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    target_percents = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"Target Variable: {target_col}\")\n",
    "    print(\"\\nCounts:\")\n",
    "    print(target_counts)\n",
    "    print(\"\\nPercentages:\")\n",
    "    print(target_percents)\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=target_col, data=df, palette=['#66b3ff', '#ff9999'])\n",
    "    plt.title(f'Distribution of {target_col}', fontsize=16)\n",
    "    plt.xlabel(target_col, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add percentages above bars\n",
    "    for i, p in enumerate(target_percents):\n",
    "        plt.text(i, target_counts.values[i] + 500, f'{p:.1f}%', \n",
    "                 ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target variable 'loan_default' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304acaab",
   "metadata": {},
   "source": [
    "## 5. Univariate Analysis of Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ff930",
   "metadata": {},
   "source": [
    "### 5.1 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "if 'loan_default' in numerical_cols:\n",
    "    numerical_cols.remove('loan_default')  # Remove target if it's numerical\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "if numerical_cols:\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i, col in enumerate(numerical_cols[:12], 1):  # Limit to first 12 columns if many\n",
    "        plt.subplot(4, 3, i)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}', fontsize=12)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If there are more than 12 numerical columns, plot the rest\n",
    "    if len(numerical_cols) > 12:\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, col in enumerate(numerical_cols[12:24], 1):\n",
    "            plt.subplot(4, 3, i)\n",
    "            sns.histplot(df[col], kde=True)\n",
    "            plt.title(f'Distribution of {col}', fontsize=12)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for numerical variables to detect outliers\n",
    "if numerical_cols:\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i, col in enumerate(numerical_cols[:12], 1):\n",
    "        plt.subplot(4, 3, i)\n",
    "        sns.boxplot(y=df[col])\n",
    "        plt.title(f'Boxplot of {col}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If there are more than 12 numerical columns, plot the rest\n",
    "    if len(numerical_cols) > 12:\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, col in enumerate(numerical_cols[12:24], 1):\n",
    "            plt.subplot(4, 3, i)\n",
    "            sns.boxplot(y=df[col])\n",
    "            plt.title(f'Boxplot of {col}', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5086f15",
   "metadata": {},
   "source": [
    "### 5.2 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Plot bar charts for categorical variables\n",
    "if categorical_cols:\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        value_counts = df[col].value_counts().sort_values(ascending=False)\n",
    "        \n",
    "        # If too many categories, only plot the top 20\n",
    "        if len(value_counts) > 20:\n",
    "            value_counts = value_counts.head(20)\n",
    "            plt.title(f'Top 20 Categories in {col}', fontsize=16)\n",
    "        else:\n",
    "            plt.title(f'Distribution of {col}', fontsize=16)\n",
    "            \n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add count labels on top of bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            plt.text(i, v + 5, str(v), ha='center')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print percentage distribution\n",
    "        print(f\"\\nPercentage distribution of {col}:\")\n",
    "        print(df[col].value_counts(normalize=True).sort_values(ascending=False) * 100)\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"No categorical columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f5b63",
   "metadata": {},
   "source": [
    "## 6. Bivariate Analysis (Relationship with Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b962c",
   "metadata": {},
   "source": [
    "### 6.1 Numerical Features vs. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if target exists and is binary\n",
    "if 'loan_default' in df.columns:\n",
    "    target = 'loan_default'\n",
    "    \n",
    "    # For numerical features vs target\n",
    "    if numerical_cols:\n",
    "        for col in numerical_cols[:6]:  # Limit to first 6 features for brevity\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create violin plots\n",
    "            sns.violinplot(x=target, y=col, data=df, palette=['#66b3ff', '#ff9999'])\n",
    "            plt.title(f'Distribution of {col} by {target}', fontsize=16)\n",
    "            plt.xlabel(target, fontsize=12)\n",
    "            plt.ylabel(col, fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print group statistics\n",
    "            print(f\"\\nStatistics of {col} grouped by {target}:\")\n",
    "            print(df.groupby(target)[col].describe())\n",
    "            print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Target variable 'loan_default' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9dcc6",
   "metadata": {},
   "source": [
    "### 6.2 Categorical Features vs. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical features vs target\n",
    "if 'loan_default' in df.columns and categorical_cols:\n",
    "    target = 'loan_default'\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # Create a cross-tabulation of the categorical variable and target\n",
    "        crosstab = pd.crosstab(df[col], df[target])\n",
    "        \n",
    "        # Calculate percentages\n",
    "        crosstab_percent = pd.crosstab(df[col], df[target], normalize='index') * 100\n",
    "        \n",
    "        # Plot stacked bar chart\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        crosstab_percent.plot(kind='bar', stacked=True, \n",
    "                              color=['#66b3ff', '#ff9999'], \n",
    "                              figsize=(12, 6))\n",
    "        \n",
    "        plt.title(f'Percentage of {target} by {col}', fontsize=16)\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel(f'Percentage', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title=target)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print crosstab and chi-square test results\n",
    "        print(f\"\\nCross-tabulation of {col} vs {target}:\")\n",
    "        print(crosstab)\n",
    "        print(\"\\nPercentage within each category:\")\n",
    "        print(crosstab_percent)\n",
    "        \n",
    "        # Perform chi-square test to check if there's a significant relationship\n",
    "        chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "        print(f\"\\nChi-square test: chi2 = {chi2:.4f}, p-value = {p:.4f}\")\n",
    "        if p < 0.05:\n",
    "            print(f\"There is a significant relationship between {col} and {target} (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"No significant relationship between {col} and {target} (p >= 0.05)\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9c26a",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "if numerical_cols:\n",
    "    # Include the target if it's numerical or convert it if it's binary\n",
    "    if 'loan_default' in df.columns:\n",
    "        corr_data = df[numerical_cols + ['loan_default']]\n",
    "    else:\n",
    "        corr_data = df[numerical_cols]\n",
    "    \n",
    "    corr_matrix = corr_data.corr()\n",
    "    \n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', \n",
    "                linewidths=0.5, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If target exists, show features most correlated with target\n",
    "    if 'loan_default' in df.columns and 'loan_default' in corr_matrix.columns:\n",
    "        target_corr = corr_matrix['loan_default'].drop('loan_default').sort_values(ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=target_corr.values, y=target_corr.index, palette='viridis')\n",
    "        plt.title('Features Correlation with Target (loan_default)', fontsize=16)\n",
    "        plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top correlated features\n",
    "        print(\"Top features positively correlated with loan_default:\")\n",
    "        print(target_corr.head(10))\n",
    "        \n",
    "        print(\"\\nTop features negatively correlated with loan_default:\")\n",
    "        print(target_corr.tail(10).sort_values())\n",
    "else:\n",
    "    print(\"No numerical columns found for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01a43c",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81841b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the EDA, we can suggest some feature engineering ideas\n",
    "# This cell doesn't actually create these features but demonstrates how they could be created\n",
    "\n",
    "# Example feature engineering (uncomment and modify to use)\n",
    "# 1. Create a debt-to-income ratio feature if not already present\n",
    "if 'debt_to_income' not in df.columns and all(col in df.columns for col in ['total_debt', 'annual_income']):\n",
    "    print(\"Creating debt-to-income ratio feature...\")\n",
    "    df['debt_to_income'] = df['total_debt'] / df['annual_income']\n",
    "    print(\"Feature created.\")\n",
    "else:\n",
    "    print(\"Either debt_to_income already exists or required columns are not present.\")\n",
    "\n",
    "# 2. Create loan amount to income ratio if applicable columns exist\n",
    "if all(col in df.columns for col in ['loan_amount', 'annual_income']):\n",
    "    print(\"\\nCreating loan-to-income ratio feature...\")\n",
    "    df['loan_to_income'] = df['loan_amount'] / df['annual_income']\n",
    "    print(\"Feature created.\")\n",
    "else:\n",
    "    print(\"\\nCannot create loan-to-income ratio: required columns not found.\")\n",
    "\n",
    "# 3. Binning numerical features like credit score, income, etc.\n",
    "if 'credit_score' in df.columns:\n",
    "    print(\"\\nCreating credit score bins...\")\n",
    "    df['credit_score_bin'] = pd.cut(df['credit_score'], \n",
    "                                   bins=[0, 580, 670, 740, 800, float('inf')],\n",
    "                                   labels=['Very Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "    print(\"Feature created.\")\n",
    "    print(df['credit_score_bin'].value_counts())\n",
    "else:\n",
    "    print(\"\\nCannot create credit score bins: credit_score column not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13173660",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165ad22",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "Based on our exploratory data analysis, here are the key findings:\n",
    "\n",
    "1. **Data Overview**: \n",
    "   - The dataset contains information about loans and their default status.\n",
    "   - [Describe actual shape and features once examined]\n",
    "\n",
    "2. **Target Variable**:\n",
    "   - The target variable is loan_default (0 = no default, 1 = default).\n",
    "   - [Note the class distribution once examined - is it imbalanced?]\n",
    "\n",
    "3. **Important Features**:\n",
    "   - [List key features that showed strong correlation with the target]\n",
    "   - [Note any patterns observed in the analysis]\n",
    "\n",
    "4. **Data Quality**:\n",
    "   - [Summarize findings about missing values]\n",
    "   - [Summarize findings about outliers]\n",
    "\n",
    "5. **Relationships**:\n",
    "   - [Summarize key relationships between features and target]\n",
    "   - [Note any interesting patterns in categorical variables]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Handle missing values through imputation or removal\n",
    "   - Handle outliers through capping, removal, or transformation\n",
    "   - Convert categorical variables to numerical through encoding techniques\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Create the suggested engineered features\n",
    "   - Consider additional interaction features between correlated variables\n",
    "\n",
    "3. **Model Development**:\n",
    "   - Implement class balancing techniques if target is imbalanced\n",
    "   - Split data into training and testing sets\n",
    "   - Train various models (Logistic Regression, Random Forest, XGBoost, etc.)\n",
    "   - Tune hyperparameters for optimal performance\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Evaluate models using appropriate metrics (AUC-ROC, F1-Score, Precision, Recall)\n",
    "   - Perform cross-validation to ensure model robustness\n",
    "\n",
    "5. **Model Interpretation**:\n",
    "   - Use feature importance and SHAP values to explain model predictions\n",
    "   - Create visualizations for model interpretation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
