{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef6337c",
   "metadata": {},
   "source": [
    "# Loan Default Prediction - Model Training\n",
    "\n",
    "This notebook focuses on training and evaluating different machine learning models for loan default prediction, based on the insights gained from our exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13872039",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = os.path.join('..', 'data', 'Loan_Default.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2024dc",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features and target\n",
    "target_column = 'Status'\n",
    "feature_columns = [col for col in df.columns if col != target_column]\n",
    "\n",
    "# Check target variable distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=target_column, data=df)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"Target value counts:\")\n",
    "print(df[target_column].value_counts())\n",
    "print(f\"Target distribution (percentage): {df[target_column].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percent = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "missing_df = pd.concat([missing_values, missing_percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(missing_df[missing_df['Total'] > 0])\n",
    "\n",
    "# We'll use SimpleImputer in our preprocessing pipeline later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b95e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Number of numeric features: {len(numeric_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed testing set shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "joblib.dump(preprocessor, os.path.join(models_dir, 'preprocessor.joblib'))\n",
    "print(\"Preprocessor saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762814fe",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf26c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve if applicable\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the trained model and metrics\n",
    "    return model, {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a34290",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model, lr_metrics = evaluate_model(lr_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c97b5d",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6685f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model, rf_metrics = evaluate_model(rf_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba90b32",
   "metadata": {},
   "source": [
    "### 3.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Gradient Boosting model...\")\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model, gb_metrics = evaluate_model(gb_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed029704",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model, xgb_metrics = evaluate_model(xgb_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133cbf7",
   "metadata": {},
   "source": [
    "### 3.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM model...\")\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "lgbm_model, lgbm_metrics = evaluate_model(lgbm_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37d1fb",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec38f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all model metrics\n",
    "all_metrics = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'Gradient Boosting': gb_metrics,\n",
    "    'XGBoost': xgb_metrics,\n",
    "    'LightGBM': lgbm_metrics\n",
    "}\n",
    "\n",
    "# Create a dataframe to compare models\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot accuracy, precision, recall, and F1 score\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    metrics_df[metric].plot(kind='bar')\n",
    "    plt.title(f'{metric.capitalize()}')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc34b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on F1 score\n",
    "best_model_name = metrics_df['f1'].idxmax()\n",
    "print(f\"Best model based on F1 score: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174731db",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model for hyperparameter tuning\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "elif best_model_name == 'Random Forest':\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    model = XGBClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model_name == 'LightGBM':\n",
    "    model = LGBMClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'num_leaves': [31, 63, 127],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search with cross-validation\n",
    "print(f\"Running GridSearchCV for {best_model_name}...\")\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, \n",
    "    cv=StratifiedKFold(n_splits=5), \n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee096d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Evaluating best {best_model_name} model with tuned hyperparameters:\")\n",
    "tuned_model, tuned_metrics = evaluate_model(best_model, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b1147",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4871619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from preprocessor\n",
    "def get_feature_names(column_transformer):\n",
    "    output_features = []\n",
    "    \n",
    "    for name, pipe, features in column_transformer.transformers_:\n",
    "        if name == 'num':\n",
    "            output_features.extend(features)\n",
    "        elif name == 'cat':\n",
    "            cats = pipe.named_steps['onehot'].get_feature_names_out(features)\n",
    "            output_features.extend(cats)\n",
    "    \n",
    "    return output_features\n",
    "\n",
    "# Try to get feature names\n",
    "try:\n",
    "    feature_names = get_feature_names(preprocessor)\n",
    "    print(f\"Number of features after preprocessing: {len(feature_names)}\")\n",
    "except:\n",
    "    feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "    print(f\"Could not get feature names, using generic feature names instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fde9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance if the best model has this attribute\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the top 20 most important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importance')\n",
    "    plt.bar(range(min(20, len(importances))), importances[indices][:20], align='center')\n",
    "    plt.xticks(range(min(20, len(importances))), [feature_names[i] for i in indices][:20], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the top 20 most important features\n",
    "    print(\"Top 20 most important features:\")\n",
    "    for i in range(min(20, len(importances))):\n",
    "        print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    coefs = best_model.coef_[0]\n",
    "    indices = np.argsort(np.abs(coefs))[::-1]\n",
    "    \n",
    "    # Plot the top 20 most important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importance (Absolute Coefficients)')\n",
    "    plt.bar(range(min(20, len(coefs))), np.abs(coefs[indices])[:20], align='center')\n",
    "    plt.xticks(range(min(20, len(coefs))), [feature_names[i] for i in indices][:20], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the top 20 most important features\n",
    "    print(\"Top 20 most important features:\")\n",
    "    for i in range(min(20, len(coefs))):\n",
    "        print(f\"{i+1}. {feature_names[indices[i]]}: {coefs[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e85473",
   "metadata": {},
   "source": [
    "## 7. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save with the model name\n",
    "model_filename = best_model_name.lower().replace(' ', '_') + '.joblib'\n",
    "joblib.dump(best_model, os.path.join(models_dir, model_filename))\n",
    "\n",
    "# Also save as best_model.joblib for easier reference\n",
    "joblib.dump(best_model, os.path.join(models_dir, 'best_model.joblib'))\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2eeca",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Preprocessed the Loan Default dataset, handling missing values and encoding categorical features\n",
    "2. Trained and evaluated several machine learning models\n",
    "3. Performed hyperparameter tuning on the best model\n",
    "4. Analyzed feature importance to understand which factors most influence loan default\n",
    "5. Saved the best model for deployment\n",
    "\n",
    "The best model was the {best_model_name} with an F1 score of {tuned_metrics['f1']:.4f} on the test set.\n",
    "\n",
    "Next steps:\n",
    "1. Deploy the model using the provided Streamlit application\n",
    "2. Monitor the model's performance over time\n",
    "3. Consider feature engineering or advanced techniques to further improve performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
